{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "#with open('transfer_experiment.json', 'r') as fp:\n",
    "#    data = json.load(fp)\n",
    "    \n",
    "def load_data(experiments):\n",
    "    data = { 'results' : {} }\n",
    "    for item in experiments:\n",
    "        experiment_title = item['id'] + '_' + item['source'] + '_' + item['target']\n",
    "        if os.path.isfile('experiments/' + experiment_title + '/' + experiment_title + '.json'):\n",
    "            with open('experiments/' + experiment_title + '/' + experiment_title + '.json', 'r') as fp:\n",
    "                results = json.load(fp)\n",
    "                data['results'][experiment_title] = results\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Results for AUC ROC"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Transfer Learning</th>\n",
       "      <th>Transfer Learning with Revision Theory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_imdb_uwcse</td>\n",
       "      <td>0.952 +/- 0.000</td>\n",
       "      <td>0.948 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2_uwcse_imdb</td>\n",
       "      <td>1.000 +/- 0.000</td>\n",
       "      <td>1.000 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6_uwcse_imdb</td>\n",
       "      <td>0.907 +/- 0.000</td>\n",
       "      <td>0.907 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7_imdb_cora</td>\n",
       "      <td>0.573 +/- 0.000</td>\n",
       "      <td>0.606 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9_imdb_cora</td>\n",
       "      <td>0.635 +/- 0.000</td>\n",
       "      <td>0.606 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12_uwcse_cora</td>\n",
       "      <td>0.950 +/- 0.000</td>\n",
       "      <td>0.942 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13_uwcse_cora</td>\n",
       "      <td>0.700 +/- 0.000</td>\n",
       "      <td>0.536 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16_yeast_twitter</td>\n",
       "      <td>0.725 +/- 0.000</td>\n",
       "      <td>0.725 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23_twitter_yeast</td>\n",
       "      <td>0.653 +/- 0.000</td>\n",
       "      <td>0.660 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35_nell_sports_nell_finances</td>\n",
       "      <td>0.622 +/- 0.000</td>\n",
       "      <td>0.641 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>37_nell_sports_nell_finances</td>\n",
       "      <td>0.812 +/- 0.000</td>\n",
       "      <td>0.852 +/- 0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Experiment Transfer Learning  \\\n",
       "0                   1_imdb_uwcse   0.952 +/- 0.000   \n",
       "1                   2_uwcse_imdb   1.000 +/- 0.000   \n",
       "2                   6_uwcse_imdb   0.907 +/- 0.000   \n",
       "3                    7_imdb_cora   0.573 +/- 0.000   \n",
       "4                    9_imdb_cora   0.635 +/- 0.000   \n",
       "5                  12_uwcse_cora   0.950 +/- 0.000   \n",
       "6                  13_uwcse_cora   0.700 +/- 0.000   \n",
       "7               16_yeast_twitter   0.725 +/- 0.000   \n",
       "8               23_twitter_yeast   0.653 +/- 0.000   \n",
       "9   35_nell_sports_nell_finances   0.622 +/- 0.000   \n",
       "10  37_nell_sports_nell_finances   0.812 +/- 0.000   \n",
       "\n",
       "   Transfer Learning with Revision Theory  \n",
       "0                         0.948 +/- 0.000  \n",
       "1                         1.000 +/- 0.000  \n",
       "2                         0.907 +/- 0.000  \n",
       "3                         0.606 +/- 0.000  \n",
       "4                         0.606 +/- 0.000  \n",
       "5                         0.942 +/- 0.000  \n",
       "6                         0.536 +/- 0.000  \n",
       "7                         0.725 +/- 0.000  \n",
       "8                         0.660 +/- 0.000  \n",
       "9                         0.641 +/- 0.000  \n",
       "10                        0.852 +/- 0.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Results for AUC PR"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Transfer Learning</th>\n",
       "      <th>Transfer Learning with Revision Theory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_imdb_uwcse</td>\n",
       "      <td>0.929 +/- 0.000</td>\n",
       "      <td>0.912 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2_uwcse_imdb</td>\n",
       "      <td>1.000 +/- 0.000</td>\n",
       "      <td>1.000 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6_uwcse_imdb</td>\n",
       "      <td>0.845 +/- 0.000</td>\n",
       "      <td>0.845 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7_imdb_cora</td>\n",
       "      <td>0.640 +/- 0.000</td>\n",
       "      <td>0.655 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9_imdb_cora</td>\n",
       "      <td>0.945 +/- 0.000</td>\n",
       "      <td>0.942 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12_uwcse_cora</td>\n",
       "      <td>0.968 +/- 0.000</td>\n",
       "      <td>0.968 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13_uwcse_cora</td>\n",
       "      <td>0.954 +/- 0.000</td>\n",
       "      <td>0.919 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16_yeast_twitter</td>\n",
       "      <td>0.704 +/- 0.000</td>\n",
       "      <td>0.690 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23_twitter_yeast</td>\n",
       "      <td>0.646 +/- 0.000</td>\n",
       "      <td>0.676 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35_nell_sports_nell_finances</td>\n",
       "      <td>0.660 +/- 0.000</td>\n",
       "      <td>0.708 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>37_nell_sports_nell_finances</td>\n",
       "      <td>0.783 +/- 0.000</td>\n",
       "      <td>0.850 +/- 0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Experiment Transfer Learning  \\\n",
       "0                   1_imdb_uwcse   0.929 +/- 0.000   \n",
       "1                   2_uwcse_imdb   1.000 +/- 0.000   \n",
       "2                   6_uwcse_imdb   0.845 +/- 0.000   \n",
       "3                    7_imdb_cora   0.640 +/- 0.000   \n",
       "4                    9_imdb_cora   0.945 +/- 0.000   \n",
       "5                  12_uwcse_cora   0.968 +/- 0.000   \n",
       "6                  13_uwcse_cora   0.954 +/- 0.000   \n",
       "7               16_yeast_twitter   0.704 +/- 0.000   \n",
       "8               23_twitter_yeast   0.646 +/- 0.000   \n",
       "9   35_nell_sports_nell_finances   0.660 +/- 0.000   \n",
       "10  37_nell_sports_nell_finances   0.783 +/- 0.000   \n",
       "\n",
       "   Transfer Learning with Revision Theory  \n",
       "0                         0.912 +/- 0.000  \n",
       "1                         1.000 +/- 0.000  \n",
       "2                         0.845 +/- 0.000  \n",
       "3                         0.655 +/- 0.000  \n",
       "4                         0.942 +/- 0.000  \n",
       "5                         0.968 +/- 0.000  \n",
       "6                         0.919 +/- 0.000  \n",
       "7                         0.690 +/- 0.000  \n",
       "8                         0.676 +/- 0.000  \n",
       "9                         0.708 +/- 0.000  \n",
       "10                        0.850 +/- 0.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Results for CLL"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Transfer Learning</th>\n",
       "      <th>Transfer Learning with Revision Theory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_imdb_uwcse</td>\n",
       "      <td>-0.234 +/- 0.000</td>\n",
       "      <td>-0.240 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2_uwcse_imdb</td>\n",
       "      <td>-0.113 +/- 0.000</td>\n",
       "      <td>-0.113 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6_uwcse_imdb</td>\n",
       "      <td>-0.324 +/- 0.000</td>\n",
       "      <td>-0.310 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7_imdb_cora</td>\n",
       "      <td>-0.659 +/- 0.000</td>\n",
       "      <td>-0.658 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9_imdb_cora</td>\n",
       "      <td>-0.281 +/- 0.000</td>\n",
       "      <td>-0.349 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12_uwcse_cora</td>\n",
       "      <td>-0.363 +/- 0.000</td>\n",
       "      <td>-0.343 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13_uwcse_cora</td>\n",
       "      <td>-0.282 +/- 0.000</td>\n",
       "      <td>-0.330 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16_yeast_twitter</td>\n",
       "      <td>-0.617 +/- 0.000</td>\n",
       "      <td>-0.612 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23_twitter_yeast</td>\n",
       "      <td>-0.656 +/- 0.000</td>\n",
       "      <td>-0.653 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35_nell_sports_nell_finances</td>\n",
       "      <td>-0.621 +/- 0.000</td>\n",
       "      <td>-0.710 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>37_nell_sports_nell_finances</td>\n",
       "      <td>-0.521 +/- 0.000</td>\n",
       "      <td>-0.499 +/- 0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Experiment Transfer Learning  \\\n",
       "0                   1_imdb_uwcse  -0.234 +/- 0.000   \n",
       "1                   2_uwcse_imdb  -0.113 +/- 0.000   \n",
       "2                   6_uwcse_imdb  -0.324 +/- 0.000   \n",
       "3                    7_imdb_cora  -0.659 +/- 0.000   \n",
       "4                    9_imdb_cora  -0.281 +/- 0.000   \n",
       "5                  12_uwcse_cora  -0.363 +/- 0.000   \n",
       "6                  13_uwcse_cora  -0.282 +/- 0.000   \n",
       "7               16_yeast_twitter  -0.617 +/- 0.000   \n",
       "8               23_twitter_yeast  -0.656 +/- 0.000   \n",
       "9   35_nell_sports_nell_finances  -0.621 +/- 0.000   \n",
       "10  37_nell_sports_nell_finances  -0.521 +/- 0.000   \n",
       "\n",
       "   Transfer Learning with Revision Theory  \n",
       "0                        -0.240 +/- 0.000  \n",
       "1                        -0.113 +/- 0.000  \n",
       "2                        -0.310 +/- 0.000  \n",
       "3                        -0.658 +/- 0.000  \n",
       "4                        -0.349 +/- 0.000  \n",
       "5                        -0.343 +/- 0.000  \n",
       "6                        -0.330 +/- 0.000  \n",
       "7                        -0.612 +/- 0.000  \n",
       "8                        -0.653 +/- 0.000  \n",
       "9                        -0.710 +/- 0.000  \n",
       "10                       -0.499 +/- 0.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Results for Recall"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Transfer Learning</th>\n",
       "      <th>Transfer Learning with Revision Theory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_imdb_uwcse</td>\n",
       "      <td>1.000 +/- 0.000</td>\n",
       "      <td>0.992 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2_uwcse_imdb</td>\n",
       "      <td>1.000 +/- 0.000</td>\n",
       "      <td>1.000 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6_uwcse_imdb</td>\n",
       "      <td>1.000 +/- 0.000</td>\n",
       "      <td>1.000 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7_imdb_cora</td>\n",
       "      <td>0.487 +/- 0.000</td>\n",
       "      <td>0.494 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9_imdb_cora</td>\n",
       "      <td>1.000 +/- 0.000</td>\n",
       "      <td>0.965 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12_uwcse_cora</td>\n",
       "      <td>0.879 +/- 0.000</td>\n",
       "      <td>0.890 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13_uwcse_cora</td>\n",
       "      <td>1.000 +/- 0.000</td>\n",
       "      <td>0.963 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16_yeast_twitter</td>\n",
       "      <td>0.594 +/- 0.000</td>\n",
       "      <td>0.513 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23_twitter_yeast</td>\n",
       "      <td>0.456 +/- 0.000</td>\n",
       "      <td>0.490 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35_nell_sports_nell_finances</td>\n",
       "      <td>0.265 +/- 0.000</td>\n",
       "      <td>0.500 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>37_nell_sports_nell_finances</td>\n",
       "      <td>0.851 +/- 0.000</td>\n",
       "      <td>0.862 +/- 0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Experiment Transfer Learning  \\\n",
       "0                   1_imdb_uwcse   1.000 +/- 0.000   \n",
       "1                   2_uwcse_imdb   1.000 +/- 0.000   \n",
       "2                   6_uwcse_imdb   1.000 +/- 0.000   \n",
       "3                    7_imdb_cora   0.487 +/- 0.000   \n",
       "4                    9_imdb_cora   1.000 +/- 0.000   \n",
       "5                  12_uwcse_cora   0.879 +/- 0.000   \n",
       "6                  13_uwcse_cora   1.000 +/- 0.000   \n",
       "7               16_yeast_twitter   0.594 +/- 0.000   \n",
       "8               23_twitter_yeast   0.456 +/- 0.000   \n",
       "9   35_nell_sports_nell_finances   0.265 +/- 0.000   \n",
       "10  37_nell_sports_nell_finances   0.851 +/- 0.000   \n",
       "\n",
       "   Transfer Learning with Revision Theory  \n",
       "0                         0.992 +/- 0.000  \n",
       "1                         1.000 +/- 0.000  \n",
       "2                         1.000 +/- 0.000  \n",
       "3                         0.494 +/- 0.000  \n",
       "4                         0.965 +/- 0.000  \n",
       "5                         0.890 +/- 0.000  \n",
       "6                         0.963 +/- 0.000  \n",
       "7                         0.513 +/- 0.000  \n",
       "8                         0.490 +/- 0.000  \n",
       "9                         0.500 +/- 0.000  \n",
       "10                        0.862 +/- 0.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Results for F1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Transfer Learning</th>\n",
       "      <th>Transfer Learning with Revision Theory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_imdb_uwcse</td>\n",
       "      <td>0.942 +/- 0.000</td>\n",
       "      <td>0.941 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2_uwcse_imdb</td>\n",
       "      <td>1.000 +/- 0.000</td>\n",
       "      <td>1.000 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6_uwcse_imdb</td>\n",
       "      <td>0.915 +/- 0.000</td>\n",
       "      <td>0.915 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7_imdb_cora</td>\n",
       "      <td>0.438 +/- 0.000</td>\n",
       "      <td>0.465 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9_imdb_cora</td>\n",
       "      <td>0.964 +/- 0.000</td>\n",
       "      <td>0.945 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12_uwcse_cora</td>\n",
       "      <td>0.875 +/- 0.000</td>\n",
       "      <td>0.891 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13_uwcse_cora</td>\n",
       "      <td>0.964 +/- 0.000</td>\n",
       "      <td>0.944 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16_yeast_twitter</td>\n",
       "      <td>0.635 +/- 0.000</td>\n",
       "      <td>0.616 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23_twitter_yeast</td>\n",
       "      <td>0.491 +/- 0.000</td>\n",
       "      <td>0.561 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35_nell_sports_nell_finances</td>\n",
       "      <td>nan +/- nan</td>\n",
       "      <td>0.544 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>37_nell_sports_nell_finances</td>\n",
       "      <td>0.783 +/- 0.000</td>\n",
       "      <td>0.797 +/- 0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Experiment Transfer Learning  \\\n",
       "0                   1_imdb_uwcse   0.942 +/- 0.000   \n",
       "1                   2_uwcse_imdb   1.000 +/- 0.000   \n",
       "2                   6_uwcse_imdb   0.915 +/- 0.000   \n",
       "3                    7_imdb_cora   0.438 +/- 0.000   \n",
       "4                    9_imdb_cora   0.964 +/- 0.000   \n",
       "5                  12_uwcse_cora   0.875 +/- 0.000   \n",
       "6                  13_uwcse_cora   0.964 +/- 0.000   \n",
       "7               16_yeast_twitter   0.635 +/- 0.000   \n",
       "8               23_twitter_yeast   0.491 +/- 0.000   \n",
       "9   35_nell_sports_nell_finances       nan +/- nan   \n",
       "10  37_nell_sports_nell_finances   0.783 +/- 0.000   \n",
       "\n",
       "   Transfer Learning with Revision Theory  \n",
       "0                         0.941 +/- 0.000  \n",
       "1                         1.000 +/- 0.000  \n",
       "2                         0.915 +/- 0.000  \n",
       "3                         0.465 +/- 0.000  \n",
       "4                         0.945 +/- 0.000  \n",
       "5                         0.891 +/- 0.000  \n",
       "6                         0.944 +/- 0.000  \n",
       "7                         0.616 +/- 0.000  \n",
       "8                         0.561 +/- 0.000  \n",
       "9                         0.544 +/- 0.000  \n",
       "10                        0.797 +/- 0.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Results for Precision"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Transfer Learning</th>\n",
       "      <th>Transfer Learning with Revision Theory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_imdb_uwcse</td>\n",
       "      <td>0.889 +/- 0.000</td>\n",
       "      <td>0.895 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2_uwcse_imdb</td>\n",
       "      <td>1.000 +/- 0.000</td>\n",
       "      <td>1.000 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6_uwcse_imdb</td>\n",
       "      <td>0.845 +/- 0.000</td>\n",
       "      <td>0.845 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7_imdb_cora</td>\n",
       "      <td>0.810 +/- 0.000</td>\n",
       "      <td>0.773 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9_imdb_cora</td>\n",
       "      <td>0.930 +/- 0.000</td>\n",
       "      <td>0.929 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12_uwcse_cora</td>\n",
       "      <td>0.902 +/- 0.000</td>\n",
       "      <td>0.909 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13_uwcse_cora</td>\n",
       "      <td>0.930 +/- 0.000</td>\n",
       "      <td>0.929 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16_yeast_twitter</td>\n",
       "      <td>0.683 +/- 0.000</td>\n",
       "      <td>0.772 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23_twitter_yeast</td>\n",
       "      <td>0.728 +/- 0.000</td>\n",
       "      <td>0.685 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35_nell_sports_nell_finances</td>\n",
       "      <td>nan +/- nan</td>\n",
       "      <td>0.642 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>37_nell_sports_nell_finances</td>\n",
       "      <td>0.724 +/- 0.000</td>\n",
       "      <td>0.740 +/- 0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Experiment Transfer Learning  \\\n",
       "0                   1_imdb_uwcse   0.889 +/- 0.000   \n",
       "1                   2_uwcse_imdb   1.000 +/- 0.000   \n",
       "2                   6_uwcse_imdb   0.845 +/- 0.000   \n",
       "3                    7_imdb_cora   0.810 +/- 0.000   \n",
       "4                    9_imdb_cora   0.930 +/- 0.000   \n",
       "5                  12_uwcse_cora   0.902 +/- 0.000   \n",
       "6                  13_uwcse_cora   0.930 +/- 0.000   \n",
       "7               16_yeast_twitter   0.683 +/- 0.000   \n",
       "8               23_twitter_yeast   0.728 +/- 0.000   \n",
       "9   35_nell_sports_nell_finances       nan +/- nan   \n",
       "10  37_nell_sports_nell_finances   0.724 +/- 0.000   \n",
       "\n",
       "   Transfer Learning with Revision Theory  \n",
       "0                         0.895 +/- 0.000  \n",
       "1                         1.000 +/- 0.000  \n",
       "2                         0.845 +/- 0.000  \n",
       "3                         0.773 +/- 0.000  \n",
       "4                         0.929 +/- 0.000  \n",
       "5                         0.909 +/- 0.000  \n",
       "6                         0.929 +/- 0.000  \n",
       "7                         0.772 +/- 0.000  \n",
       "8                         0.685 +/- 0.000  \n",
       "9                         0.642 +/- 0.000  \n",
       "10                        0.740 +/- 0.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Results for Learning and Revision time"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Transfer Learning</th>\n",
       "      <th>Transfer Learning with Revision Theory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_imdb_uwcse</td>\n",
       "      <td>0.881 +/- 0.000</td>\n",
       "      <td>7.239 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2_uwcse_imdb</td>\n",
       "      <td>1.281 +/- 0.000</td>\n",
       "      <td>5.258 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6_uwcse_imdb</td>\n",
       "      <td>0.836 +/- 0.000</td>\n",
       "      <td>3.709 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7_imdb_cora</td>\n",
       "      <td>2.247 +/- 0.000</td>\n",
       "      <td>124.275 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9_imdb_cora</td>\n",
       "      <td>1.286 +/- 0.000</td>\n",
       "      <td>13.815 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12_uwcse_cora</td>\n",
       "      <td>28.738 +/- 0.000</td>\n",
       "      <td>452.003 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13_uwcse_cora</td>\n",
       "      <td>2.494 +/- 0.000</td>\n",
       "      <td>38.781 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16_yeast_twitter</td>\n",
       "      <td>2.295 +/- 0.000</td>\n",
       "      <td>23.182 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23_twitter_yeast</td>\n",
       "      <td>4.764 +/- 0.000</td>\n",
       "      <td>48.013 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35_nell_sports_nell_finances</td>\n",
       "      <td>1.665 +/- 0.000</td>\n",
       "      <td>12.034 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>37_nell_sports_nell_finances</td>\n",
       "      <td>8.721 +/- 0.000</td>\n",
       "      <td>37.197 +/- 0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Experiment Transfer Learning  \\\n",
       "0                   1_imdb_uwcse   0.881 +/- 0.000   \n",
       "1                   2_uwcse_imdb   1.281 +/- 0.000   \n",
       "2                   6_uwcse_imdb   0.836 +/- 0.000   \n",
       "3                    7_imdb_cora   2.247 +/- 0.000   \n",
       "4                    9_imdb_cora   1.286 +/- 0.000   \n",
       "5                  12_uwcse_cora  28.738 +/- 0.000   \n",
       "6                  13_uwcse_cora   2.494 +/- 0.000   \n",
       "7               16_yeast_twitter   2.295 +/- 0.000   \n",
       "8               23_twitter_yeast   4.764 +/- 0.000   \n",
       "9   35_nell_sports_nell_finances   1.665 +/- 0.000   \n",
       "10  37_nell_sports_nell_finances   8.721 +/- 0.000   \n",
       "\n",
       "   Transfer Learning with Revision Theory  \n",
       "0                         7.239 +/- 0.000  \n",
       "1                         5.258 +/- 0.000  \n",
       "2                         3.709 +/- 0.000  \n",
       "3                       124.275 +/- 0.000  \n",
       "4                        13.815 +/- 0.000  \n",
       "5                       452.003 +/- 0.000  \n",
       "6                        38.781 +/- 0.000  \n",
       "7                        23.182 +/- 0.000  \n",
       "8                        48.013 +/- 0.000  \n",
       "9                        12.034 +/- 0.000  \n",
       "10                       37.197 +/- 0.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Results for Inference time"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Transfer Learning</th>\n",
       "      <th>Transfer Learning with Revision Theory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_imdb_uwcse</td>\n",
       "      <td>0.727 +/- 0.000</td>\n",
       "      <td>0.745 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2_uwcse_imdb</td>\n",
       "      <td>0.950 +/- 0.000</td>\n",
       "      <td>0.948 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6_uwcse_imdb</td>\n",
       "      <td>0.657 +/- 0.000</td>\n",
       "      <td>0.561 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7_imdb_cora</td>\n",
       "      <td>2.219 +/- 0.000</td>\n",
       "      <td>5.638 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9_imdb_cora</td>\n",
       "      <td>1.463 +/- 0.000</td>\n",
       "      <td>2.544 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12_uwcse_cora</td>\n",
       "      <td>19.117 +/- 0.000</td>\n",
       "      <td>72.640 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13_uwcse_cora</td>\n",
       "      <td>3.688 +/- 0.000</td>\n",
       "      <td>61.122 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16_yeast_twitter</td>\n",
       "      <td>1.124 +/- 0.000</td>\n",
       "      <td>1.473 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23_twitter_yeast</td>\n",
       "      <td>2.577 +/- 0.000</td>\n",
       "      <td>2.990 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35_nell_sports_nell_finances</td>\n",
       "      <td>0.944 +/- 0.000</td>\n",
       "      <td>0.793 +/- 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>37_nell_sports_nell_finances</td>\n",
       "      <td>2.601 +/- 0.000</td>\n",
       "      <td>2.180 +/- 0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Experiment Transfer Learning  \\\n",
       "0                   1_imdb_uwcse   0.727 +/- 0.000   \n",
       "1                   2_uwcse_imdb   0.950 +/- 0.000   \n",
       "2                   6_uwcse_imdb   0.657 +/- 0.000   \n",
       "3                    7_imdb_cora   2.219 +/- 0.000   \n",
       "4                    9_imdb_cora   1.463 +/- 0.000   \n",
       "5                  12_uwcse_cora  19.117 +/- 0.000   \n",
       "6                  13_uwcse_cora   3.688 +/- 0.000   \n",
       "7               16_yeast_twitter   1.124 +/- 0.000   \n",
       "8               23_twitter_yeast   2.577 +/- 0.000   \n",
       "9   35_nell_sports_nell_finances   0.944 +/- 0.000   \n",
       "10  37_nell_sports_nell_finances   2.601 +/- 0.000   \n",
       "\n",
       "   Transfer Learning with Revision Theory  \n",
       "0                         0.745 +/- 0.000  \n",
       "1                         0.948 +/- 0.000  \n",
       "2                         0.561 +/- 0.000  \n",
       "3                         5.638 +/- 0.000  \n",
       "4                         2.544 +/- 0.000  \n",
       "5                        72.640 +/- 0.000  \n",
       "6                        61.122 +/- 0.000  \n",
       "7                         1.473 +/- 0.000  \n",
       "8                         2.990 +/- 0.000  \n",
       "9                         0.793 +/- 0.000  \n",
       "10                        2.180 +/- 0.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiments = [\n",
    "            {'id': '1', 'source':'imdb', 'target':'uwcse', 'predicate':'workedunder', 'to_predicate':'advisedby'},\n",
    "            {'id': '2', 'source':'uwcse', 'target':'imdb', 'predicate':'advisedby', 'to_predicate':'workedunder'},\n",
    "            {'id': '3', 'source':'imdb', 'target':'uwcse', 'predicate':'movie', 'to_predicate':'publication'},\n",
    "            {'id': '4', 'source':'uwcse', 'target':'imdb', 'predicate':'publication', 'to_predicate':'movie'},\n",
    "            {'id': '5', 'source':'imdb', 'target':'uwcse', 'predicate':'genre', 'to_predicate':'inphase'},\n",
    "            {'id': '6', 'source':'uwcse', 'target':'imdb', 'predicate':'inphase', 'to_predicate':'genre'},\n",
    "            {'id': '7', 'source':'imdb', 'target':'cora', 'predicate':'workedunder', 'to_predicate':'samevenue'},\n",
    "            {'id': '8', 'source':'imdb', 'target':'cora', 'predicate':'workedunder', 'to_predicate':'samebib'},\n",
    "            {'id': '9', 'source':'imdb', 'target':'cora', 'predicate':'workedunder', 'to_predicate':'sameauthor'},\n",
    "            {'id': '10', 'source':'imdb', 'target':'cora', 'predicate':'workedunder', 'to_predicate':'sametitle'},\n",
    "            {'id': '11', 'source':'uwcse', 'target':'cora', 'predicate':'advisedby', 'to_predicate':'samevenue'},\n",
    "            {'id': '12', 'source':'uwcse', 'target':'cora', 'predicate':'advisedby', 'to_predicate':'samebib'},\n",
    "            {'id': '13', 'source':'uwcse', 'target':'cora', 'predicate':'advisedby', 'to_predicate':'sameauthor'},\n",
    "            {'id': '14', 'source':'uwcse', 'target':'cora', 'predicate':'advisedby', 'to_predicate':'sametitle'},\n",
    "            {'id': '15', 'source':'yeast', 'target':'twitter', 'predicate':'proteinclass', 'to_predicate':'accounttype'},\n",
    "            {'id': '16', 'source':'yeast', 'target':'twitter', 'predicate':'interaction', 'to_predicate':'follows'},\n",
    "            {'id': '17', 'source':'yeast', 'target':'twitter', 'predicate':'location', 'to_predicate':'tweets'},\n",
    "            {'id': '18', 'source':'yeast', 'target':'twitter', 'predicate':'enzyme', 'to_predicate':'tweets'},\n",
    "            {'id': '19', 'source':'yeast', 'target':'twitter', 'predicate':'function', 'to_predicate':'tweets'},\n",
    "            {'id': '20', 'source':'yeast', 'target':'twitter', 'predicate':'phenotype', 'to_predicate':'tweets'},\n",
    "            {'id': '21', 'source':'yeast', 'target':'twitter', 'predicate':'complex', 'to_predicate':'tweets'},\n",
    "            {'id': '22', 'source':'twitter', 'target':'yeast', 'predicate':'accounttype', 'to_predicate':'proteinclass'},\n",
    "            {'id': '23', 'source':'twitter', 'target':'yeast', 'predicate':'follows', 'to_predicate':'interaction'},\n",
    "            {'id': '24', 'source':'twitter', 'target':'yeast', 'predicate':'tweets', 'to_predicate':'location'},\n",
    "            {'id': '25', 'source':'twitter', 'target':'yeast', 'predicate':'tweets', 'to_predicate':'enzyme'},\n",
    "            {'id': '26', 'source':'twitter', 'target':'yeast', 'predicate':'tweets', 'to_predicate':'function'},\n",
    "            {'id': '27', 'source':'twitter', 'target':'yeast', 'predicate':'tweets', 'to_predicate':'phenotype'},\n",
    "            {'id': '28', 'source':'twitter', 'target':'yeast', 'predicate':'tweets', 'to_predicate':'complex'},\n",
    "            {'id': '29', 'source':'nell_sports', 'target':'nell_finances', 'predicate':'teamalsoknownas', 'to_predicate':'companyalsoknownas'},\n",
    "            {'id': '30', 'source':'nell_sports', 'target':'nell_finances', 'predicate':'teamplaysagainstteam', 'to_predicate':'companyalsoknownas'},\n",
    "            {'id': '31', 'source':'nell_sports', 'target':'nell_finances', 'predicate':'teamplaysagainstteam', 'to_predicate':'acquired'},\n",
    "            {'id': '32', 'source':'nell_sports', 'target':'nell_finances', 'predicate':'teamplaysagainstteam', 'to_predicate':'bankboughtbank'},\n",
    "            {'id': '33', 'source':'nell_sports', 'target':'nell_finances', 'predicate':'athleteplayssport', 'to_predicate':'companyceo'},\n",
    "            {'id': '34', 'source':'nell_sports', 'target':'nell_finances', 'predicate':'athleteplayssport', 'to_predicate':'bankchiefexecutiveceo'},\n",
    "            {'id': '35', 'source':'nell_sports', 'target':'nell_finances', 'predicate':'athleteplaysforteam', 'to_predicate':'bankchiefexecutiveceo'},\n",
    "            {'id': '36', 'source':'nell_sports', 'target':'nell_finances', 'predicate':'athleteplaysforteam', 'to_predicate':'companyceo'},\n",
    "            {'id': '37', 'source':'nell_sports', 'target':'nell_finances', 'predicate':'teamplayssport', 'to_predicate':'companyeconomicsector'},\n",
    "            {'id': '38', 'source':'nell_finances', 'target':'nell_sports', 'predicate':'companyalsoknownas', 'to_predicate':'teamalsoknownas'},\n",
    "            {'id': '39', 'source':'nell_finances', 'target':'nell_sports', 'predicate':'companyalsoknownas', 'to_predicate':'teamplaysagainstteam'},\n",
    "            {'id': '40', 'source':'nell_finances', 'target':'nell_sports', 'predicate':'acquired', 'to_predicate':'teamplaysagainstteam'},\n",
    "            {'id': '41', 'source':'nell_finances', 'target':'nell_sports', 'predicate':'bankboughtbank', 'to_predicate':'teamplaysagainstteam'},\n",
    "            {'id': '42', 'source':'nell_finances', 'target':'nell_sports', 'predicate':'companyceo', 'to_predicate':'athleteplayssport'},\n",
    "            {'id': '43', 'source':'nell_finances', 'target':'nell_sports', 'predicate':'bankchiefexecutiveceo', 'to_predicate':'athleteplayssport'},\n",
    "            {'id': '44', 'source':'nell_finances', 'target':'nell_sports', 'predicate':'bankchiefexecutiveceo', 'to_predicate':'athleteplaysforteam'},\n",
    "            {'id': '45', 'source':'nell_finances', 'target':'nell_sports', 'predicate':'companyceo', 'to_predicate':'athleteplaysforteam'},\n",
    "            {'id': '46', 'source':'nell_finances', 'target':'nell_sports', 'predicate':'companyeconomicsector', 'to_predicate':'teamplayssport'},\n",
    "            {'id': '47', 'source':'yeast', 'target':'facebook', 'predicate':'interaction', 'to_predicate':'edge'},\n",
    "            {'id': '48', 'source':'twitter', 'target':'facebook', 'predicate':'follows', 'to_predicate':'edge'},\n",
    "            {'id': '49', 'source':'imdb', 'target':'facebook', 'predicate':'workedunder', 'to_predicate':'edge'},\n",
    "            {'id': '50', 'source':'uwcse', 'target':'facebook', 'predicate':'advisedby', 'to_predicate':'edge'},\n",
    "]\n",
    "\n",
    "data = load_data(experiments)\n",
    "\n",
    "for metric in ['AUC ROC', 'AUC PR', 'CLL', 'Recall', 'F1', 'Precision', 'Learning and Revision time', 'Inference time']:\n",
    "    display(Markdown('# Results for ' + metric))\n",
    "    table = []\n",
    "    for j in range(len(experiments)):\n",
    "        dataset = experiments[j]['id'] + '_' + experiments[j]['source'] + '_' + experiments[j]['target']\n",
    "        if dataset in data['results']: \n",
    "            #for metric in ['AUC ROC', 'AUC PR']:\n",
    "                #display(Markdown('## ' + metric))\n",
    "            mapping_time = ''\n",
    "            if metric == 'Precision':\n",
    "                #scratch = np.array([(np.array([item['rdn_b']['Precision'][0] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                #scratch_rdn = np.array([(np.array([item['rdn']['Precision'][0] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                parameter = np.array([(np.array([item['transfer']['parameter']['Precision'][0] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                transfer = np.array([(np.array([item['transfer']['Precision'][0] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "            elif metric == 'Learning and Revision time':\n",
    "                #scratch = np.array([(np.array([item['rdn_b']['Learning time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                #scratch_rdn = np.array([(np.array([item['rdn']['Learning time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                parameter = np.array([(np.array([item['transfer']['parameter']['Learning time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                transfer = np.array([(np.array([item['transfer']['Learning time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                #mapping = np.array([(np.array([item['transfer']['Mapping results']['Total time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                #mapping_time = ' (%.1f)' % mapping.mean()\n",
    "            elif metric == 'Inference time':\n",
    "                #scratch = np.array([(np.array([item['rdn_b']['Inference time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                #scratch_rdn = np.array([(np.array([item['rdn']['Inference time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                parameter = np.array([(np.array([item['transfer']['parameter']['Inference time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                transfer = np.array([(np.array([item['transfer']['Inference time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "            else:\n",
    "                #scratch = np.array([(np.array([item['rdn_b'][metric] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                #scratch_rdn = np.array([(np.array([item['rdn'][metric] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                parameter = np.array([(np.array([item['transfer']['parameter'][metric] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                transfer = np.array([(np.array([item['transfer'][metric] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "            table.append([dataset, '%.3f +/- %.3f' % (parameter.mean(), 2 * parameter.std()),\n",
    "            '%.3f +/- %.3f' % (transfer.mean(), 2 * transfer.std()),\n",
    "            #'%.3f +/- %.3f' % (scratch.mean(), 2 * scratch.std()),\n",
    "            #'%.3f +/- %.3f' % (scratch_rdn.mean(), 2 * scratch_rdn.std())])\n",
    "                          ])\n",
    "    #display(pd.DataFrame(table, columns=['Experiment', 'Transfer Learning', 'Transfer Learning with Revision Theory', 'Learning from scratch (RDN-B)', 'Learning from scratch (RDN)']))\n",
    "    display(pd.DataFrame(table, columns=['Experiment', 'Transfer Learning', 'Transfer Learning with Revision Theory']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Results for 1_imdb_uwcse"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## AUC ROC"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-35ea24e7aec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mtr_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                     \u001b[0msc_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rdn_b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                     \u001b[0msc_rdn_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rdn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                     \u001b[0mpr_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transfer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parameter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for j in range(len(experiments)):\n",
    "    dataset = experiments[j]['id'] + '_' + experiments[j]['source'] + '_' + experiments[j]['target']\n",
    "    if dataset in data['results']: \n",
    "        display(Markdown('# Results for ' + dataset))\n",
    "    table = []\n",
    "    for metric in ['AUC ROC', 'AUC PR', 'CLL', 'Recall', 'F1', 'Precision', 'Learning and Revision time', 'Inference time']:\n",
    "    #for metric in ['AUC ROC', 'AUC PR']:\n",
    "        if dataset in data['results']: \n",
    "            #for metric in ['AUC ROC', 'AUC PR']:\n",
    "            display(Markdown('## ' + metric))\n",
    "            if metric == 'Precision':\n",
    "                for m in range(len(data['results'][str(dataset)])):\n",
    "                    print('Experiment %s' % (m+1))\n",
    "                    sc = [item['rdn_b']['Precision'][0] for item in data['results'][str(dataset)][m]]\n",
    "                    sc_rdn = [item['rdn']['Precision'][0] for item in data['results'][str(dataset)][m]]\n",
    "                    pr = [item['transfer']['parameter']['Precision'][0] for item in data['results'][str(dataset)][m]]\n",
    "                    tr = [item['transfer']['Precision'][0] for item in data['results'][str(dataset)][m]] \n",
    "                    print('Scratch: %s' % sc)\n",
    "                    print('Transfer: %s' % pr)\n",
    "                    print('Transfer & Revision: %s' % tr) \n",
    "                    print('\\n')\n",
    "                #scratch = np.array([item['scratch']['Precision'][0] for sublist in data['results'][str(dataset)] for item in sublist])\n",
    "                #parameter = np.array([item['transfer']['Parameter Learning results']['Precision'][0] for sublist in data['results'][str(dataset)] for item in sublist])\n",
    "                #transfer = np.array([item['transfer']['Precision'][0] for sublist in data['results'][str(dataset)] for item in sublist])\n",
    "            elif metric == 'Learning and Revision time':\n",
    "                for m in range(len(data['results'][str(dataset)])):\n",
    "                    print('Experiment %s' % (m+1))\n",
    "                    sc = [item['rdn_b']['Learning time'] for item in data['results'][str(dataset)][m]]\n",
    "                    sc_rdn = [item['rdn']['Learning time'] for item in data['results'][str(dataset)][m]]\n",
    "                    pr = ['%s (%.1f)' % (item['transfer']['parameter']['Learning time'], item['transfer']['Mapping results']['Total time']) for item in data['results'][str(dataset)][m]]\n",
    "                    tr = ['%s (%.1f)' % (item['transfer']['Learning time'], item['transfer']['Mapping results']['Total time']) for item in data['results'][str(dataset)][m]] \n",
    "                    print('Scratch: %s' % sc)\n",
    "                    print('Transfer: %s' % pr)\n",
    "                    print('Transfer & Revision: %s' % tr) \n",
    "                    print('\\n')\n",
    "                #scratch = np.array([item['scratch']['Learning time'] for sublist in data['results'][str(dataset)] for item in sublist])\n",
    "                #parameter = np.array([item['transfer']['Parameter Learning results']['Learning time'] for sublist in data['results'][str(dataset)] for item in sublist])\n",
    "                #transfer = np.array([item['transfer']['Learning time'] for sublist in data['results'][str(dataset)] for item in sublist])\n",
    "            elif metric == 'Inference time':\n",
    "                for m in range(len(data['results'][str(dataset)])):\n",
    "                    print('Experiment %s' % (m+1))\n",
    "                    sc = [item['rdn_b']['Inference time'] for item in data['results'][str(dataset)][m]]\n",
    "                    sc_rdn = [item['rdn']['Inference time'] for item in data['results'][str(dataset)][m]]\n",
    "                    pr = [item['transfer']['parameter']['Inference time'] for item in data['results'][str(dataset)][m]]\n",
    "                    tr = [item['transfer']['Inference time'] for item in data['results'][str(dataset)][m]] \n",
    "                    print('Scratch: %s' % sc)\n",
    "                    print('Transfer: %s' % pr)\n",
    "                    print('Transfer & Revision: %s' % tr) \n",
    "                    print('\\n')\n",
    "                #scratch = np.array([item['scratch']['Inference time'] for sublist in data['results'][str(dataset)] for item in sublist])\n",
    "                #parameter = np.array([item['transfer']['Parameter Learning results']['Learning time'] for sublist in data['results'][str(dataset)] for item in sublist])\n",
    "                #transfer = np.array([item['transfer']['Inference time'] for sublist in data['results'][str(dataset)] for item in sublist])\n",
    "            else:\n",
    "                sc_sum = np.zeros(len(data['results'][str(dataset)][0]))\n",
    "                sc_rdn_sum = np.zeros(len(data['results'][str(dataset)][0]))\n",
    "                pr_sum = np.zeros(len(data['results'][str(dataset)][0]))\n",
    "                tr_sum = np.zeros(len(data['results'][str(dataset)][0]))\n",
    "                for m in range(len(data['results'][str(dataset)][0])):\n",
    "                    sc_sum += np.array([item['rdn_b'][metric] for item in data['results'][str(dataset)][m]])\n",
    "                    sc_rdn_sum += np.array([item['rdn'][metric] for item in data['results'][str(dataset)][m]])\n",
    "                    pr_sum += np.array([item['transfer']['parameter'][metric] for item in data['results'][str(dataset)][m]])\n",
    "                    tr_sum += np.array([item['transfer'][metric] for item in data['results'][str(dataset)][m]])\n",
    "                sc_sum /= len(data['results'][str(dataset)])\n",
    "                sc_rdn_sum /= len(data['results'][str(dataset)])\n",
    "                pr_sum /= len(data['results'][str(dataset)])\n",
    "                tr_sum /= len(data['results'][str(dataset)])\n",
    "                pvalue_table = []\n",
    "                pvalue = stats.ttest_rel(tr_sum,sc_sum)\n",
    "                pvalue2 = stats.ttest_rel(tr_sum,sc_rdn_sum)\n",
    "                #print(pvalue)\n",
    "                pvalue_table.append(['transferRDN-B Ref', '%.3f' % (pvalue[1]), '%.3f' % (pvalue2[1])])\n",
    "                pvalue = stats.ttest_rel(pr_sum, sc_sum)\n",
    "                pvalue2 = stats.ttest_rel(pr_sum, sc_rdn_sum)\n",
    "                #print(pvalue)\n",
    "                pvalue_table.append(['transferRDN-B', '%.3f' % (pvalue[1]), '%.3f' % (pvalue2[1])])\n",
    "                display(pd.DataFrame(pvalue_table, columns=['p-value', 'RDN-B', 'RDN']))\n",
    "                \n",
    "                for m in range(len(data['results'][str(dataset)])):\n",
    "                    print('Experiment %s' % (m+1))\n",
    "                    sc = [item['rdn_b'][metric] for item in data['results'][str(dataset)][m]]\n",
    "                    sc_rdn = [item['rdn'][metric] for item in data['results'][str(dataset)][m]]\n",
    "                    pr = [item['transfer']['parameter'][metric] for item in data['results'][str(dataset)][m]]\n",
    "                    tr = [item['transfer'][metric] for item in data['results'][str(dataset)][m]] \n",
    "                    print('Scratch RDN-B: %s' % sc)\n",
    "                    print('Scratch RDN: %s' % sc_rdn)\n",
    "                    print('Transfer: %s' % pr)\n",
    "                    print('Transfer & Revision: %s' % tr) \n",
    "                    print('\\n')\n",
    "                #scratch = np.array([item['scratch'][metric] for sublist in data['results'][str(dataset)] for item in sublist])\n",
    "                #parameter = np.array([item['transfer']['Parameter Learning results'][metric] for sublist in data['results'][str(dataset)] for item in sublist])\n",
    "                #transfer = np.array([item['transfer'][metric] for sublist in data['results'][str(dataset)] for item in sublist])\n",
    "            #print('\\n')\n",
    "            #print('Scratch: %s' % scratch)\n",
    "            #print('Transfer: %s' % parameter)\n",
    "            #print('Transfer & Revision: %s' % transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Results for 1_imdb_uwcse"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'Mapping results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-941cc40eecd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parameter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transfer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parameter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Learning time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transfer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transfer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Learning time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transfer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Mapping results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Total time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mapping_time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'(%.1f)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Inference time'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-941cc40eecd9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parameter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transfer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parameter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Learning time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transfer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transfer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Learning time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transfer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Mapping results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Total time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mapping_time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'(%.1f)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Inference time'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-941cc40eecd9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parameter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transfer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parameter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Learning time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transfer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transfer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Learning time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transfer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Mapping results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Total time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mapping_time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'(%.1f)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Inference time'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Mapping results'"
     ]
    }
   ],
   "source": [
    "for j in range(len(experiments)):\n",
    "    table = []\n",
    "    dataset = experiments[j]['id'] + '_' + experiments[j]['source'] + '_' + experiments[j]['target']\n",
    "    display(Markdown('# Results for ' + dataset))\n",
    "    if dataset in data['results']:\n",
    "        values = {'scratch': {}, 'scratch_rdn': {}, 'parameter': {}, 'transfer': {}, 'mapping_time': {}}\n",
    "        for metric in ['CLL', 'AUC ROC', 'AUC PR', 'Learning and Revision time']:\n",
    "            mapping_time = ''\n",
    "            if metric == 'Precision':\n",
    "                values['scratch'][metric] = np.array([(np.array([item['rdn_b']['Precision'][0] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                values['scratch_rdn'][metric] = np.array([(np.array([item['rdn']['Precision'][0] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                values['parameter'][metric] = np.array([(np.array([item['transfer']['parameter']['Precision'][0] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                values['transfer'][metric] = np.array([(np.array([item['transfer']['Precision'][0] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "            elif metric == 'Learning and Revision time':\n",
    "                values['scratch'][metric] = np.array([(np.array([item['rdn_b']['Learning time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                values['scratch_rdn'][metric] = np.array([(np.array([item['rdn']['Learning time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                values['parameter'][metric] = np.array([(np.array([item['transfer']['parameter']['Learning time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                values['transfer'][metric] = np.array([(np.array([item['transfer']['Learning time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                mapping = np.array([(np.array([item['transfer']['Mapping results']['Total time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                values['mapping_time'] = '(%.1f)' % mapping.mean()\n",
    "            elif metric == 'Inference time':\n",
    "                values['scratch'][metric] = np.array([(np.array([item['rdn_b']['Inference time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                values['scratch_rdn'][metric] = np.array([(np.array([item['rdn']['Inference time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                values['parameter'][metric] = np.array([(np.array([item['transfer']['parameter']['Inference time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                values['transfer'][metric] = np.array([(np.array([item['transfer']['Inference time'] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "            else:\n",
    "                values['scratch'][metric] = np.array([(np.array([item['rdn_b'][metric] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                values['scratch_rdn'][metric] = np.array([(np.array([item['rdn'][metric] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                values['parameter'][metric] = np.array([(np.array([item['transfer']['parameter'][metric] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "                values['transfer'][metric] = np.array([(np.array([item['transfer'][metric] for item in sublist])).mean() for sublist in data['results'][str(dataset)]])\n",
    "\n",
    "        print('RDN         & %.3f & %.3f & %.3f & %.3f \\\\\\\\' % (values['scratch_rdn']['CLL'].mean(), values['scratch_rdn']['AUC ROC'].mean(), values['scratch_rdn']['AUC PR'].mean(), values['scratch_rdn']['Learning and Revision time'].mean()))\n",
    "        print('RDN-B       & %.3f & %.3f & %.3f & %.3f \\\\\\\\' % (values['scratch']['CLL'].mean(), values['scratch']['AUC ROC'].mean(), values['scratch']['AUC PR'].mean(), values['scratch']['Learning and Revision time'].mean()))\n",
    "        print('trRDN-B     & %.3f & %.3f & %.3f & %.3f%s \\\\\\\\' % (values['parameter']['CLL'].mean(), values['parameter']['AUC ROC'].mean(), values['parameter']['AUC PR'].mean(), values['parameter']['Learning and Revision time'].mean(), values['mapping_time']))\n",
    "        print('trRDN-B Ref & %.3f & %.3f & %.3f & %.3f%s \\\\\\\\' % (values['transfer']['CLL'].mean(), values['transfer']['AUC ROC'].mean(), values['transfer']['AUC PR'].mean(), values['transfer']['Learning and Revision time'].mean(), values['mapping_time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
